diff --git a/sam2/modeling/sam/transformer.py b/sam2/modeling/sam/transformer.py
index 0000000..0000000 100644
--- a/sam2/modeling/sam/transformer.py
+++ b/sam2/modeling/sam/transformer.py
@@
     def forward(
         self, q: Tensor, k: Tensor, v: Tensor, num_k_exclude_rope: int = 0
     ) -> Tensor:
         # Input projections
-        q = self.q_proj(q.to(self.q_proj.weight.dtype))
-        k = self.k_proj(k.to(self.k_proj.weight.dtype))
-        v = self.v_proj(v.to(self.v_proj.weight.dtype))
+        q = self.q_proj(
+            q.to(dtype=self.q_proj.weight.dtype, device=self.q_proj.weight.device)
+        )
+        k = self.k_proj(
+            k.to(dtype=self.k_proj.weight.dtype, device=self.k_proj.weight.device)
+        )
+        v = self.v_proj(
+            v.to(dtype=self.v_proj.weight.dtype, device=self.v_proj.weight.device)
+        )
@@
         dropout_p = self.dropout_p if self.training else 0.0
         # Attention
-        if q.dtype != k.dtype:
-            k = k.to(dtype=q.dtype)
-        if q.dtype != v.dtype:
-            v = v.to(dtype=q.dtype)
+        if q.dtype != k.dtype or q.device != k.device:
+            k = k.to(dtype=q.dtype, device=q.device)
+        if q.dtype != v.dtype or q.device != v.device:
+            v = v.to(dtype=q.dtype, device=q.device)
         out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)
 
         out = self._recombine_heads(out)
         out = self.out_proj(out)
 
         return out

