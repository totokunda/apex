{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d504f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "inputs_1 = torch.load(\"/home/tosinkuye/apex/si_inputs1.pt\")\n",
    "inputs_2 = torch.load(\"/home/tosinkuye/apex/denoise_inputs1.pt\")\n",
    "inputs_2.pop('attention_kwargs')\n",
    "\n",
    "for k, v in inputs_1.items():\n",
    "    print(k, v.shape, v.dtype)\n",
    "    \n",
    "for k, v in inputs_2.items():\n",
    "    print(k, v.shape, v.dtype)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer.wan.base.model import WanTransformer3DModel\n",
    "from src.converters.transformer_converters import WanTransformerConverter\n",
    "\n",
    "extra_path = '/mnt/localssd/apex-diffusion/components/BowenXue/Stand-In/resolve/main/Stand-In_wan2.1_T2V_14B_ver1.0.ckpt'\n",
    "extra_weights = torch.load(extra_path)\n",
    "\n",
    "converter = WanTransformerConverter()\n",
    "converter.convert(extra_weights)\n",
    "\n",
    "model = WanTransformer3DModel.from_pretrained(\"/mnt/localssd/apex-diffusion/components/Wan-AI_Wan2.1-T2V-14B-Diffusers/transformer\", torch_dtype=torch.bfloat16)\n",
    "model.init_ip_projections(device='meta', dtype=torch.bfloat16)\n",
    "model.load_state_dict(extra_weights, strict=False, assign=True)\n",
    "model.config.ip_adapter = True\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c66788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "sys.path.append(\"/home/tosinkuye/apex/Stand-In\")\n",
    "from models import ModelManager\n",
    "from models.set_condition_branch import set_stand_in\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "paths = glob(\"/mnt/localssd/checkpoints/base_model/*.safetensors\")\n",
    "print(paths)\n",
    "\n",
    "model_manager = ModelManager()\n",
    "model_manager.load_model(\n",
    "    paths,\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "dit = model_manager.fetch_model('wan_video_dit', index=2)\n",
    "\n",
    "@dataclass\n",
    "class FakePipe:\n",
    "    dit: Any\n",
    "\n",
    "fake_pipe = FakePipe(dit=dit)\n",
    "set_stand_in(fake_pipe, model_path=extra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.wan_video_dit import WanModel, RMSNorm, sinusoidal_embedding_1d\n",
    "timestep = inputs_1['timestep']\n",
    "timestep_ip = torch.zeros_like(timestep)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(dit.time_embedding[0].weight.dtype)\n",
    "    t = dit.time_embedding(sinusoidal_embedding_1d(dit.freq_dim, timestep))\n",
    "    t_mod = dit.time_projection(t).unflatten(1, (6, dit.dim))\n",
    "    t_ip = dit.time_embedding(sinusoidal_embedding_1d(dit.freq_dim, timestep_ip))\n",
    "    t_mod_ip = dit.time_projection(t_ip).unflatten(1, (6, dit.dim))\n",
    "    print(t_mod_ip.shape, t_mod.shape)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    condition_embedder = model.condition_embedder\n",
    "    temb = condition_embedder.time_embedder(condition_embedder.timesteps_proj(timestep)).type_as(t_mod)\n",
    "    timestep_proj = condition_embedder.time_proj(condition_embedder.act_fn(temb)).unflatten(1, (6, dit.dim))\n",
    "    temb_ip = condition_embedder.time_embedder(condition_embedder.timesteps_proj(timestep_ip)).type_as(t_mod_ip)\n",
    "    timestep_proj_ip = condition_embedder.time_proj(condition_embedder.act_fn(temb_ip)).unflatten(1, (6, dit.dim))\n",
    "    print(timestep_proj.shape, timestep_proj_ip.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = inputs_1['latents']\n",
    "hidden_states_ip = inputs_1['ip_image']\n",
    "\n",
    "hidden_states_2 = inputs_2['latents']\n",
    "hidden_states_ip_2 = inputs_2['ip_image']\n",
    "\n",
    "torch.testing.assert_close(hidden_states, hidden_states_2)\n",
    "torch.testing.assert_close(hidden_states_ip, hidden_states_ip_2)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    pe_x, (f, h, w) = dit.patchify(hidden_states)\n",
    "    pe_x_ip, (f_ip, h_ip, w_ip) = dit.patchify(hidden_states_ip)\n",
    "    print(pe_x.shape, f, h, w)\n",
    "    print(pe_x_ip.shape, f_ip, h_ip, w_ip)\n",
    "        \n",
    "with torch.no_grad():\n",
    "    pe_hidden_states = model.patch_embedding(hidden_states_2).flatten(2).transpose(1, 2)\n",
    "    pe_hidden_states_ip = model.patch_embedding(hidden_states_ip_2).flatten(2).transpose(1, 2)\n",
    "    print(pe_hidden_states.shape, f, h, w)\n",
    "    print(pe_hidden_states_ip.shape, f_ip, h_ip, w_ip)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = inputs_1['context']\n",
    "\n",
    "with torch.no_grad():\n",
    "    context_embeddings = dit.text_embedding(context)\n",
    "    print(context_embeddings.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_hidden_states = model.condition_embedder.text_embedder(context)\n",
    "    print(encoder_hidden_states.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    offset = 1\n",
    "    freqs = (\n",
    "            torch.cat(\n",
    "                [\n",
    "                    dit.freqs[0][offset : f + offset]\n",
    "                    .view(f, 1, 1, -1)\n",
    "                    .expand(f, h, w, -1),\n",
    "                    dit.freqs[1][offset : h + offset]\n",
    "                    .view(1, h, 1, -1)\n",
    "                    .expand(f, h, w, -1),\n",
    "                    dit.freqs[2][offset : w + offset]\n",
    "                    .view(1, 1, w, -1)\n",
    "                    .expand(f, h, w, -1),\n",
    "                ],\n",
    "                dim=-1,\n",
    "            )\n",
    "            .reshape(f * h * w, 1, -1)\n",
    "            .to(hidden_states.device)\n",
    "        )\n",
    "\n",
    "    freqs_ip = (\n",
    "                torch.cat(\n",
    "                    [\n",
    "                        dit.freqs[0][0]\n",
    "                        .view(f_ip, 1, 1, -1)\n",
    "                        .expand(f_ip, h_ip, w_ip, -1),\n",
    "                        dit.freqs[1][h + offset : h + offset + h_ip]\n",
    "                        .view(1, h_ip, 1, -1)\n",
    "                        .expand(f_ip, h_ip, w_ip, -1),\n",
    "                        dit.freqs[2][w + offset : w + offset + w_ip]\n",
    "                        .view(1, 1, w_ip, -1)\n",
    "                        .expand(f_ip, h_ip, w_ip, -1),\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                )\n",
    "                .reshape(f_ip * h_ip * w_ip, 1, -1)\n",
    "                .to(hidden_states_ip.device)\n",
    "            )\n",
    "    freqs = torch.cat([freqs, freqs_ip], dim=0)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    rotary_emb = model.rope(hidden_states)\n",
    "    rotary_emb_ip = model.rope(hidden_states, hidden_states_ip, time_index=0)\n",
    "    rotary_emb = torch.cat([rotary_emb, rotary_emb_ip], dim=2)\n",
    "\n",
    "\n",
    "\n",
    "print(freqs.shape, rotary_emb.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdcf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(freqs, rotary_emb.squeeze(0).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519acfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dit_block = dit.blocks[0]\n",
    "model_block = model.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = torch.load(\"/home/tosinkuye/apex/all_transformer_inputs.pt\")\n",
    "\n",
    "dit_kwargs = model_inputs[\"dit_kwargs\"]\n",
    "model_kwargs = model_inputs[\"model_kwargs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb840d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    dit_block_output = dit_block(**dit_kwargs)\n",
    "    #dit_blk_x, dit_blk_x_ip = dit_block_output\n",
    "    #print(dit_blk_x.shape, dit_blk_x_ip.shape)\n",
    "    \n",
    "\n",
    "with torch.no_grad():\n",
    "    ip_hidden_states_len = model_kwargs['hidden_states_ip'].shape[1]\n",
    "    model_block_output = model_block(**model_kwargs)\n",
    "    #model_blk_x, model_blk_x_ip = model_block_output[:, :-ip_hidden_states_len], model_block_output[:, -ip_hidden_states_len:]\n",
    "    #print(model_blk_x.shape, model_blk_x_ip.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612937fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x1, x2):\n",
    "    diff = x1 - x2\n",
    "    print(diff.abs().max())\n",
    "    torch.testing.assert_close(x1, x2)\n",
    "\n",
    "\n",
    "compare(model_block_output, dit_block_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
